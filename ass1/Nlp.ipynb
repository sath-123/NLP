{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3359,"status":"ok","timestamp":1676739761323,"user":{"displayName":"Sathvika Mahesh","userId":"16318600982752154119"},"user_tz":-330},"id":"g8LMaAPYqckn","outputId":"7343e246-67e2-46db-bfbe-c3b21d3a6195"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","import math\n","import re\n","import random\n","from torch import nn,optim\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20617,"status":"ok","timestamp":1676730347555,"user":{"displayName":"Sathvika Mahesh","userId":"16318600982752154119"},"user_tz":-330},"id":"Jb8zuvoj6lHz","outputId":"48d55809-e7b6-45ae-e5cd-62eca1711c01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":690,"status":"ok","timestamp":1676739765662,"user":{"displayName":"Sathvika Mahesh","userId":"16318600982752154119"},"user_tz":-330},"id":"2Qdox21cvZbB"},"outputs":[],"source":["def cleaning(input):\n","    # print()\n","    input = input.lower()\n","    # URl\n","    input = re.sub(r'(https?://|www\\.|http?://)\\S+', 'URL', input)\n","    #hashtag\n","    input = re.sub(r'#\\S+', 'Hashtag', input)\n","    #mention\n","    input = re.sub(r'@\\S+', 'Mention', input)\n","    input = re.sub(r'[a-zA-Z0-9]*@\\S+', 'Email', input)\n","    #percentage\n","    input = re.sub(r'\\d+\\%','Percentage',input)\n","    # date\n","    input=re.sub(r'\\[\\s*(\\d+)\\s*\\]',' ',input)\n","    input=re.sub(r'\\d{1,2}[-/.]\\d{1,2}[/.-]\\d{2,4}','Date',input)\n","    # removing all white spaces\n","    input=re.sub(r'\\s+',' ',input)\n","    # Decimal\n","    input=re.sub(r'\\d+\\.\\d+','Decimal',input)\n","    # number\n","    # input =re.sub(r'([ \\b+ ])',' ',input)\n","    input=re.sub(r'\\d+','Number',input)\n","    # replacing double punctuations\n","    input =re.sub(r'([{*.,-/&^%!_+=}])\\1+',r'\\1',input)\n","    #\n","    input=re.sub(r'_(\\w+)_',r' \\1',input)\n","    input=re.sub(r'_(\\w+)',r' \\1',input)\n","    input=re.sub(r'(\\w+)_',r' \\1',input)\n","\n","    input=re.sub(r'—\\s+(\\w+)\\s+—',r' \\1',input)\n","    input=re.sub(r'—(\\w+)',r' \\1',input)\n","    input=re.sub(r'(\\w+)—',r' \\1',input)\n","\n","    input=re.sub(r'-(\\w+)-',r' \\1',input)\n","    input=re.sub(r'-(\\w+)',r' \\1',input)\n","    input=re.sub(r'(\\w+)-',r' \\1',input)\n","    \n","\n","    \n","    # input=re.sub(r'—(\\w+)—',r' \\1',input)\n","    # input=re.sub(r'(\\w+).',r' \\1',input)\n","    # input=re.sub(r'_you_',r'you',input\n","    input=re.sub(r'mr\\.','Mr',input)\n","    input=re.sub(r'mrs\\.','Mrs',input)\n","    input=re.sub(r'r. w. (robert william)','Robert William',input)\n","    \n","    return input\n","\n","\n","\n","# fuction for tokenization\n","def tokenization(input):\n","    # input=re.sub(r'((<URL>)|(<Percent age>))',r' \\1',input)\n","    # input=re.sub(r'(<(Percentage)>)',r'\\1',input)\n","    input=re.sub(r'\\((\\w+)\\)',r' \\1',input)\n","    input=re.sub(r'([!-_.?^*\\'{}\\~/$`:\"])',r' \\1',input)\n","    tokens = input.split()\n","    # tokens=re.findall(\"<\\w+>|\\w+|[\\.,\\\"\\?\\:\\;']\",input)\n","    return tokens\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1676739768974,"user":{"displayName":"Sathvika Mahesh","userId":"16318600982752154119"},"user_tz":-330},"id":"5uf-MAbDrpqZ"},"outputs":[],"source":["class MODEL(nn.Module): \n","  def __init__(self,traindata,batch_size):\n","    super().__init__()\n","    self.batch_size=batch_size\n","    self.prefix=[]\n","    self.maximumlen=0\n","    self.words=self.get_words(traindata)\n","    self.single_word={}\n","    self.vocab=self.unique_words()\n","    print(len(self.vocab))\n","    self.word_to_index= {word : i for i ,word in enumerate(self.vocab)}\n","    self.context_words=self.context()\n","    self.context_batch=self.batching()\n","\n","    self.hidden_size=130\n","    self.emb_dim=100\n","    self.embedding = nn.Embedding(len(self.vocab),self.emb_dim,device=device)\n","    self.lstm = nn.LSTM(input_size=self.emb_dim, hidden_size=self.hidden_size)\n","    self.answer = nn.Linear(self.hidden_size,len(self.vocab),device=device)\n","    self.to(device)\n","    \n","  def get_words(self,traindata):\n","    # it will give all words of the traindata\n","    vocab=[]\n","    maxlen=0\n","    for line in traindata:\n","      tokens=tokenization(line)\n","      length=len(tokens)\n","      for i in range(1,length):\n","        self.prefix.append(tokens[:i+1])\n","      if(maxlen<len(tokens)):\n","        maxlen=len(tokens)\n","      vocab = vocab + tokens\n","    self.maximumlen=maxlen\n","    return vocab\n","\n","  def unique_words(self):\n","    # for geeting unique words\n","    for word in self.words:\n","      if self.single_word.get(word)!=None:\n","        self.single_word[word]+=1\n","      else :\n","        self.single_word[word]=1\n","    length=len(self.words)\n","    for i in range(0,length):\n","      if self.single_word[self.words[i]]< 3:\n","        self.words[i]='<unk>'\n","    # self.words.append('<unk>')\n","    u_words=list(set(self.words))\n","    u_words.append('<unk>')\n","    u_words=list(set(u_words))\n","\n","    u_words.append('<pad>')\n","    return u_words\n","\n","\n","  def context(self):\n","    # converting prefix sequences to equal length and from word to index\n","    i=0\n","    for prefix_sequence in self.prefix:\n","      length =len(prefix_sequence)\n","      word_index=[]\n","      for word in prefix_sequence:\n","        if self.single_word[word]<3:\n","          word_index.append(self.word_to_index['<unk>'])\n","        else :\n","          word_index.append(self.word_to_index[word])\n","      self.prefix[i]=[self.word_to_index['<pad>']]*(self.maximumlen-length) + word_index\n","      i+=1\n","\n","  def batching(self):\n","    # dividing the prefix sequences into batches of batch_size\n","     self.prefix=torch.tensor(self.prefix,device=device)\n","     batch_wise=torch.split(self.prefix,self.batch_size)\n","     batchs=[]\n","     for batch in batch_wise:\n","       batchs.append((batch[:,:-1],batch[:,-1]))\n","     return batchs\n","  \n","  def forward(self,context):\n","    # for lstm model\n","    embed_output=self.embedding(context)\n","    out,state=self.lstm(embed_output)\n","    final=self.answer(out[:,-1])\n","    return final\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":371,"status":"ok","timestamp":1676739773142,"user":{"displayName":"Sathvika Mahesh","userId":"16318600982752154119"},"user_tz":-330},"id":"kSKVntGcGA5I"},"outputs":[],"source":["def training(model,L_rate,epochs):\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(),lr=L_rate)\n","  initial_loss=-math.inf\n","  for i in range(epochs):\n","    loss_for_epoch=0\n","    print(\"entered\")\n","    batchs=0\n","    for batch ,(history,prediction) in enumerate(model.context_batch):\n","      optimizer.zero_grad() # set the gradients to zero before starting to do backpropragation \n","      predicted_one=model.forward(history)\n","      loss=criterion(predicted_one,prediction)\n","      loss.backward()\n","      optimizer.step()\n","      loss_for_epoch+=loss.item()\n","      batchs+=1\n","    loss_for_epoch=loss_for_epoch/batchs\n","    if abs(initial_loss-loss_for_epoch)<=0.001:\n","      break\n","    initial_loss=loss_for_epoch\n","    \n","  print(\"done\")\n","\n","\n","\n","      \n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":579,"status":"ok","timestamp":1676739777372,"user":{"displayName":"Sathvika Mahesh","userId":"16318600982752154119"},"user_tz":-330},"id":"s6MYaBrhWM3s"},"outputs":[],"source":["def perplexity_for_sentence(model,text):\n","  # print(model.word_to_index['The'],model.word_to_index['<unk>'])\n","  text=cleaning(text)\n","  tokens=tokenization(text)\n","  # sen=tokens\n","\n","  length=len(tokens)\n","  if length > model.maximumlen or length<2:\n","    return 0,0,1\n","  # print(tokens)\n","  # print(model.word_to_index['the'],model.word_to_index['<unk>'])\n","  # print(length)\n","  for i in range(length):\n","    if tokens[i] not in model.vocab:\n","      tokens[i]='<unk>'\n","    \n","  for i in range(length):\n","    # print(tokens[i])\n","    tokens[i]=model.word_to_index[tokens[i]]\n","  prefix_sequences=[]\n","  for i in range(1,length):\n","    prefix_sequences.append(tokens[:i+1])\n","  s=0\n","  for sequence in prefix_sequences:\n","    siz=len(sequence)\n","    prefix_sequences[s]=(model.maximumlen-siz)*[model.word_to_index['<pad>']]+prefix_sequences[s]\n","    s+=1\n","  # predicted=prefix_sequences[:,-1]\n","  prefix_sequences=torch.tensor(prefix_sequences,device=device)\n","  words=prefix_sequences[:,-1]\n","  context=prefix_sequences[:,:-1]\n","  distribution=model.forward(context)\n","  # print(distribution[0])\n","  p = torch.nn.functional.softmax(distribution, dim=0).cpu().detach().numpy()\n","  # print(distribution[0][1].item())\n","  prob=1\n","  for i in range(0,s):\n","    # print(words[i])\n","    # num=model.word_to_index[words[i]]\n","    probs=p[i][words[i]]\n","    prob=prob*probs\n","    # print(probs.item())\n","  if s==0 or prob==0:\n","    return 0,0,1\n","\n","  return prob**(-1/s),prob,0\n","\n","\n","\n","\n","\n","  "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":948,"status":"ok","timestamp":1676739781226,"user":{"displayName":"Sathvika Mahesh","userId":"16318600982752154119"},"user_tz":-330},"id":"GQhTrndrc7YL"},"outputs":[],"source":["def perplexity_for_test(model,testfile,outputfile):\n","  m=0\n","  f=open(testfile,'r')\n","  inp=f.read()\n","  inp=inp.split('\\n')\n","  # print(m,\"kk\")\n","  fileout=open(outputfile,'w+')\n","  answer=''\n","  average=0\n","  s=0\n","  for line in inp:\n","    pep,p,test=perplexity_for_sentence(model,line)\n","    if test==0:\n","      answer=answer+line+'.'+' '+str(pep)+'\\n'\n","      average=average+pep\n","      s+=1\n","    \n","  # print(s,\"kkkk\")\n","  average=average/s\n","  fileout.write(str(average))\n","  fileout.write('\\n')\n","  fileout.write(answer)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FoBIttIarvYZ","outputId":"344f396c-aaf7-4695-ae4d-b11e3390d253"},"outputs":[{"name":"stdout","output_type":"stream","text":["15021\n","6902\n"]}],"source":["path='/content/drive/MyDrive/nlp/file2_train.txt'\n","################################for split##########\n","# with open(path, 'r') as f:\n","#     inp = f.read()\n","# inp=cleaning(inp)\n","# inp=list(filter(None,inp.split('.')))\n","# # inp=inp.split('.')\n","# print(len(inp))\n","# data=[]\n","# lines=0\n","# for line in inp:\n","#   lines+=1\n","#   data.append(line)\n","#########################################\n","f=open(path,'r')\n","data=[]\n","lines=0\n","for line in f.readlines():\n","  lines+=1\n","  data.append(line)\n","print(lines)\n","\n","###################################for spliting test,train,validation########\n","# trainlength=int(lines*70/100)\n","# testlength=int(lines*15/100)\n","# print(trainlength,testlength)\n","# # 3800,4613,-813\n","# random.shuffle(data)\n","# Train_data=data[:15145]\n","# Val_data=data[15145:18390]\n","# Test_data=data[-3245:]\n","# print(len(data),len(Train_data),len(Val_data),len(Test_data))\n","# trainfile = open('/content/drive/MyDrive/nlp/file2_train.txt','w+')\n","# testfile=open('/content/drive/MyDrive/nlp/file2_test.txt','w+')\n","# valfile=open('/content/drive/MyDrive/nlp/file2_val.txt','w+')\n","# spr=0\n","# for x in Train_data:\n","#   spr+=1\n","#   trainfile.write(x+'\\n')\n","#   # trainfile.write(\"\\n\")\n","# for x in Test_data:\n","#   testfile.write(x)\n","#   testfile.write(\"\\n\")\n","# for x in Val_data:\n","#   valfile.write(x)\n","#   valfile.write(\"\\n\")\n","\n","\n","###############################################end###################\n","Trained_model=MODEL(data,256)\n","print(Trained_model.prefix[1][Trained_model.maximumlen-2])\n","training(Trained_model,0.001,15)\n","torch.save(Trained_model.state_dict(),'/content/drive/MyDrive/nlp/eng_lm15model2.pth')\n","# Trained_model.load_state_dict(torch.load('/content/drive/MyDrive/nlp/eng_lm150.pth'))\n","# perplexity_for_test(Trained_model,'/content/drive/MyDrive/nlp/file1_train.txt','/content/drive/MyDrive/nlp/train.txt')\n","\n","# print(perplexity_for_sentence(Trained_model,'console lady catherine as well as you can'))\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1676644585683,"user":{"displayName":"MAHESH SATHVIKA","userId":"01885277738621414542"},"user_tz":-330},"id":"rxZrdS0mrFwT","outputId":"ef39389e-5699-425f-c691-3ac0444c41c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["216\n"]}],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
